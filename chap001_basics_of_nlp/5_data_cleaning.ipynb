{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ba399d-d57b-4b3d-abc6-741a9546d906",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Data Cleaning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f3151-e89e-45df-a492-5a85a1f91a08",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c971b60-7bee-46d2-8b0d-4119d04d7a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2421091-9b29-4dcf-aa38-e86ec8f36f1a",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37dfd19-49ef-4d3e-b69a-16941f4f734a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "data_df = pd.read_csv(r\"D:/Artificial_Intelligence/nat_lang_proc/data/SMSSpamCollection.tsv\", delimiter=\"\\t\", header=None)\n",
    "data_df.columns=[\"label\", \"body_text\"]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6901d5-0304-4740-b3b1-c5d69468b81c",
   "metadata": {},
   "source": [
    "### Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7069671b-b97b-4dec-996c-7d47dd1a3267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punc(text: str):\n",
    "    \"\"\"\n",
    "    Function which will remove punctuation from the text\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "           Takes a line of text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "    Returns a list of characters with no punctuation\n",
    "    \"\"\"\n",
    "    text_nopunc=\"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a192660-212b-4fd7-9bd3-9a600d563ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_nopunc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text   \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \\\n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                           text_nopunc  \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "2                                          Nah I dont think he goes to usf he lives around here though  \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent  \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text_nopunc\"] = data_df[\"body_text\"].apply(remove_punc)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a707a8-f0d4-40ac-a36d-04041f962d6c",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "594498e0-2484-45a0-9399-888f9a0ae11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(text: str):\n",
    "    \"\"\"\n",
    "    Tokenizes the text\n",
    "    \"\"\"\n",
    "    tokenized_text = re.findall(\"\\w+\", text)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e578e1-0c66-46d2-8b61-ac2dbe00aa20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_nopunc</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text   \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \\\n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                           text_nopunc   \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \\\n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                        tokenized_text  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"tokenized_text\"] = data_df[\"text_nopunc\"].apply(lambda x: tokenization(x.lower()))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c79869-67c6-4133-b855-d594f154852f",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d051529-5935-484c-8a99-38dd2e8cb35b",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Stop word removal is one of the most commonly used preprocessing steps across different NLP applications. The idea is simply removing the words that occur commonly across all the documents in the corpus. Typically, articles and pronouns are generally classified as stop words. These words have no significance in some of the NLP tasks like information retrieval and classification, which means these words are not very discriminative. On the contrary, in some NLP applications stop word removal will have very little impact. Most of the time, the stop word list for the given language is a well hand-curated list of words that occur most commonly across corpuses\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96c947f3-f506-412e-930d-ac21a227f440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append(r\"D:\\Artificial_Intelligence\\nat_lang_proc\\nltk_data\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d48e8e2-18ba-4337-a7f5-4672b16cf1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a335924b-c493-474e-b91e-4fdb2df9f542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_text: List) -> List:\n",
    "    '''\n",
    "    Function removes the stops words from tokenized_text list\n",
    "    '''\n",
    "    text_nostopwords = [word for word in tokenized_text if word not in stopwords]\n",
    "    return text_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6910f3a-1db7-4e74-9cb4-5d2dff263a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_nopunc</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text   \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \\\n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                           text_nopunc   \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \\\n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                        tokenized_text   \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \\\n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                          no_stopwords  \n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                                                       [date, sunday]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"no_stopwords\"] = data_df[\"tokenized_text\"].apply(remove_stopwords)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ae047-360b-43a4-ac7f-97cc2621e0e9",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006eefd8-18c7-4343-a191-fe16d5a57f64",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/\">Different tokenizers offered by the nltk module</a>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d916b5-128c-4ed0-99d3-fe4ee2ab600d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
