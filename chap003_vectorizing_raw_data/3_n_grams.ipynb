{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc99a067-001e-4522-9dd8-a9175cbe6afe",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          N-Grams</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eeb7f3-12bf-4b8d-bc67-4efa0aeb442b",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Creates a document-term matrix where count still occupy the cell but instead of the columns representing single term they will represent all combinations of adjacent words at length n in your text. <br>\n",
    "       <b>Example:</b> NLP is an intresting topic\n",
    "       <table>\n",
    "           <tr>\n",
    "               <th>n</th>\n",
    "               <th>Name</th>\n",
    "               <th>Tokens</th>\n",
    "           </tr>\n",
    "           <tr>\n",
    "               <td>2</td>\n",
    "               <td>Bigram</td>\n",
    "               <td>[\"NLP is\", \"is an\", \"an intresting\", \"intresting topic\"]</td>\n",
    "           </tr>\n",
    "           <tr>\n",
    "               <td>3</td>\n",
    "               <td>Trigram</td>\n",
    "               <td>[\"NLP is an\", \"is an intresting\", \"an intresting topic\"]</td>\n",
    "           </tr>\n",
    "           <tr>\n",
    "               <td>4</td>\n",
    "               <td>four-gram</td>\n",
    "               <td>[\"NLP is an intresting\", \"is an intresting topic\"]</td>\n",
    "           </tr>\n",
    "       </table>\n",
    "       \n",
    "   </font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7978c-c2b0-45c6-89e1-0dccbb6da444",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8863716a-318f-4537-a4d4-8a01bd57180c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dbe55-7e66-4ac3-bf0c-1c3766e488a5",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9727a959-ea2d-4f9d-b58c-7556297c0d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                          body_text\n",
       "0    ham  I've been searching for the right words to tha...\n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3    ham  Even my brother is not like to speak with me. ...\n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "nltk.data.path.append(r\"D:\\Artificial_Intelligence\\nat_lang_proc\\nltk_data\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"English\")\n",
    "\n",
    "data_df = pd.read_csv(r\"D:/Artificial_Intelligence/nat_lang_proc/data/SMSSpamCollection.tsv\", delimiter=\"\\t\", header=None)\n",
    "data_df.columns = [\"labels\", \"body_text\"]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aff29f-8be1-4e4e-98b2-f54ff5914a46",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25daccd-800a-42ee-8968-6f064e50d73a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    without_punc = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    tokenzied_text = re.findall(\"\\w+\", without_punc)\n",
    "    # NOTE: N-gram need a sentence instead of token of words\n",
    "    stemmed_text = \" \".join([ps.stem(word) for word in tokenzied_text if word not in stopwords])\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67ba6e8-4fb1-4b7b-baae-e2596fdd0af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df[\"clean_text\"] = data_df[\"body_text\"].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "625ecb9b-4252-47e3-a6c1-93a7a3fb26b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>body_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive search right word thank breather promis wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                          body_text   \n",
       "0    ham  I've been searching for the right words to tha...  \\\n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ive search right word thank breather promis wo...  \n",
       "1  free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "2          nah dont think goe usf live around though  \n",
       "3      even brother like speak treat like aid patent  \n",
       "4                                        date sunday  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf810b6-3f6d-405c-9492-1aa398b7199a",
   "metadata": {},
   "source": [
    "### N-gram Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f20345e6-aa2b-480b-866e-877642fdc6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 31275)\n",
      "['008704050406 sp' '0089mi last' '0121 2025050' ... 'üll submit'\n",
      " 'üll take' '〨ud even']\n"
     ]
    }
   ],
   "source": [
    "# ngram_range parameter takes range ngram to consider. Here we are telling to consider only Bigram\n",
    "# if you space range range from (1, 3) then it will consider unigram, bigram and trigram\n",
    "ngram_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "x_counts = ngram_vect.fit_transform(data_df[\"clean_text\"])\n",
    "print(x_counts.shape)\n",
    "print(ngram_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8c6db-1fbf-4bd9-9d88-24ae7e5249b2",
   "metadata": {},
   "source": [
    "#### N-gram on smaller data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f73a8e-f1bc-4cfb-a951-1e1f229befc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 209)\n",
      "['09061701461 claim' '100 20000' '100000 prize' '11 month' '12 hour'\n",
      " '150pday 6day' '16 tsandc' '20000 pound' '2005 text' '21st may'\n",
      " '4txtú120 poboxox36504w45wq' '6day 16' '81010 tc' '87077 eg'\n",
      " '87077 trywal' '87121 receiv' '87575 cost' '900 prize' 'aft finish'\n",
      " 'aid patent' 'anymor tonight' 'appli 08452810075over18' 'appli repli'\n",
      " 'ard smth' 'around though' 'bless time' 'breather promis' 'brother like'\n",
      " 'call 09061701461' 'call mobil' 'caller press' 'callertun caller'\n",
      " 'camera free' 'cash 100' 'chanc win' 'claim 81010' 'claim call'\n",
      " 'claim code' 'click httpwap' 'click wap' 'co free' 'code kl341'\n",
      " 'colour mobil' 'comp win' 'copi friend' 'cost 150pday' 'credit click'\n",
      " 'cri enough' 'csh11 send' 'cup final' 'custom select' 'da stock'\n",
      " 'date sunday' 'dont miss' 'dont think' 'dont want' 'eg england'\n",
      " 'eh rememb' 'england 87077' 'england macedonia' 'enough today'\n",
      " 'entitl updat' 'entri questionstd' 'entri wkli' 'even brother' 'fa 87121'\n",
      " 'fa cup' 'feel way' 'final tkt' 'fine way' 'finish lunch' 'finish ur'\n",
      " 'first lar' 'free 08002986030' 'free call' 'free entri' 'free membership'\n",
      " 'friend callertun' 'fulfil promis' 'go str' 'go tri' 'goalsteam news'\n",
      " 'goe usf' 'gonna home' 'grant fulfil' 'ha ha' 'ha joke' 'help grant'\n",
      " 'hl info' 'home soon' 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl'\n",
      " 'im gonna' 'ive cri' 'ive search' 'jackpot txt' 'kim watch' 'kl341 valid'\n",
      " 'lar da' 'latest colour' 'lccltd pobox' 'like aid' 'like speak'\n",
      " 'link next' 'live around' 'lor ard' 'lor finish' 'lunch alreadi'\n",
      " 'lunch go' 'macedonia dont' 'make wet' 'may 2005' 'mell mell' 'mell oru'\n",
      " 'membership 100000' 'messag click' 'minnaminungint nurungu'\n",
      " 'miss goalsteam' 'mobil 11' 'mobil camera' 'mobil updat' 'month entitl'\n",
      " 'month ha' 'nah dont' 'name ye' 'nation team' 'naughti make'\n",
      " 'network custom' 'news txt' 'next txt' 'nurungu vettam' 'oh kim'\n",
      " 'oru minnaminungint' 'pay first' 'per request' 'pobox 4403ldnw1a7rw18'\n",
      " 'poboxox36504w45wq 16' 'pound txt' 'press copi' 'prize jackpot'\n",
      " 'prize reward' 'promis wonder' 'promis wont' 'questionstd txt'\n",
      " 'ratetc appli' 'receiv entri' 'receivea 900' 'rememb spell' 'repli hl'\n",
      " 'request mell' 'reward claim' 'right word' 'scotland 4txtú120'\n",
      " 'search right' 'select receivea' 'send 87575' 'serious spell'\n",
      " 'set callertun' 'six chanc' 'smth lor' 'soon dont' 'speak treat'\n",
      " 'spell name' 'stock comin' 'str lor' 'stuff anymor' 'take help'\n",
      " 'talk stuff' 'tc wwwdbuknet' 'team 87077' 'text fa' 'thank breather'\n",
      " 'think goe' 'tkt 21st' 'tonight ive' 'treat like' 'tri month'\n",
      " 'trywal scotland' 'tsandc appli' 'txt csh11' 'txt messag' 'txt ratetc'\n",
      " 'txt ur' 'txt word' 'updat co' 'updat latest' 'ur lunch' 'ur nation'\n",
      " 'urgent week' 'use credit' 'usf live' 'valid 12' 'valu network'\n",
      " 'vettam set' 'want talk' 'wap link' 'way feel' 'way gota' 'week free'\n",
      " 'win cash' 'win fa' 'winner valu' 'wkli comp' 'wonder bless' 'wont take'\n",
      " 'word claim' 'word thank' 'wwwdbuknet lccltd' 'xxxmobilemovieclub use'\n",
      " 'ye naughti']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data_df[:20]\n",
    "ngram_vect_sample = CountVectorizer(ngram_range=(2, 2))\n",
    "x_counts_sample = ngram_vect_sample.fit_transform(data_sample[\"clean_text\"])\n",
    "print(x_counts_sample.shape)\n",
    "print(ngram_vect_sample.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e92a15a-8c3e-4ca7-bb09-4b48c37790b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>09061701461 claim</th>\n",
       "      <th>100 20000</th>\n",
       "      <th>100000 prize</th>\n",
       "      <th>11 month</th>\n",
       "      <th>12 hour</th>\n",
       "      <th>150pday 6day</th>\n",
       "      <th>16 tsandc</th>\n",
       "      <th>20000 pound</th>\n",
       "      <th>2005 text</th>\n",
       "      <th>21st may</th>\n",
       "      <th>...</th>\n",
       "      <th>win fa</th>\n",
       "      <th>winner valu</th>\n",
       "      <th>wkli comp</th>\n",
       "      <th>wonder bless</th>\n",
       "      <th>wont take</th>\n",
       "      <th>word claim</th>\n",
       "      <th>word thank</th>\n",
       "      <th>wwwdbuknet lccltd</th>\n",
       "      <th>xxxmobilemovieclub use</th>\n",
       "      <th>ye naughti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   09061701461 claim  100 20000  100000 prize  11 month  12 hour   \n",
       "0                  0          0             0         0        0  \\\n",
       "1                  0          0             0         0        0   \n",
       "2                  0          0             0         0        0   \n",
       "3                  0          0             0         0        0   \n",
       "4                  0          0             0         0        0   \n",
       "\n",
       "   150pday 6day  16 tsandc  20000 pound  2005 text  21st may  ...  win fa   \n",
       "0             0          0            0          0         0  ...       0  \\\n",
       "1             0          0            0          1         1  ...       1   \n",
       "2             0          0            0          0         0  ...       0   \n",
       "3             0          0            0          0         0  ...       0   \n",
       "4             0          0            0          0         0  ...       0   \n",
       "\n",
       "   winner valu  wkli comp  wonder bless  wont take  word claim  word thank   \n",
       "0            0          0             1          1           0           1  \\\n",
       "1            0          1             0          0           0           0   \n",
       "2            0          0             0          0           0           0   \n",
       "3            0          0             0          0           0           0   \n",
       "4            0          0             0          0           0           0   \n",
       "\n",
       "   wwwdbuknet lccltd  xxxmobilemovieclub use  ye naughti  \n",
       "0                  0                       0           0  \n",
       "1                  0                       0           0  \n",
       "2                  0                       0           0  \n",
       "3                  0                       0           0  \n",
       "4                  0                       0           0  \n",
       "\n",
       "[5 rows x 209 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x_counts_sample.toarray())\n",
    "df.columns = ngram_vect_sample.get_feature_names_out()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391981fe-8048-4a00-9571-8038e42f6070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>body_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive search right word thank breather promis wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                          body_text   \n",
       "0    ham  I've been searching for the right words to tha...  \\\n",
       "1   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3    ham  Even my brother is not like to speak with me. ...   \n",
       "4    ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ive search right word thank breather promis wo...  \n",
       "1  free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "2          nah dont think goe usf live around though  \n",
       "3      even brother like speak treat like aid patent  \n",
       "4                                        date sunday  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9795188-38b4-4ca9-8f27-4536fa8434ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
