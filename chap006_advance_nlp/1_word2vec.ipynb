{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47852849-02ca-4da8-89e6-f2dfcfeebf99",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Word2Vect</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923f29c-48ed-4a76-8881-22c171e2dfcc",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Word2Vec is a shallow,two-layer neural network that accepts a text corpus as an input and returns a set\n",
    "       of vectors(also know as embedding). Each vector is a neumeric representation of a given word. It is capable\n",
    "       of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "       Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving \n",
    "       Neural Networks): Skip Gram and Common Bag Of Words (CBOW).\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c18b9-04dd-476b-bbce-bb18c81c1b5d",
   "metadata": {},
   "source": [
    "### Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26a3c389-12ed-4b6e-a506-da0c22a4e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87976f0-fdc9-45ef-8422-9785ce5e7664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading pretrained word vectors using gensim\n",
    "# Other pretrained model can found at below link\n",
    "# https://github.com/RaRe-Technologies/gensim-data\n",
    "wiki_embeddings = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1309591f-628f-4b4e-998e-b6b36d9516de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the word vector king\n",
    "wiki_embeddings[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49330c3b-14e8-4404-be8d-2d764378c80b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7682328820228577),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919989585876465),\n",
       " ('kingdom', 0.6811409592628479),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712858080863953),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the words most similar to king based on trained word vector\n",
    "wiki_embeddings.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91031aea-ae5c-48e8-8ae6-451358fe987c",
   "metadata": {},
   "source": [
    "### Training our own vectorizing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75ba46-fd1a-4089-8858-75cdda850942",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Understanding with simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "647459b6-2213-4e52-937b-550e7113e679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_dummy_data = pd.Series([[\"something\", \"is\", \"better\", \"than\", \"nothing\"], \n",
    "                          [\"good\", \"is\", \"better\", \"than\", \"nothing\"],\n",
    "                          [\"Love\", \"is\", \"good\", \"but\", \"break\",\"up\", \"is\",\"better\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d68becac-0087-4103-994a-178501c05cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [something, is, better, than, nothing]\n",
       "1               [good, is, better, than, nothing]\n",
       "2    [Love, is, good, but, break, up, is, better]\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "100f2571-d6e1-4cf2-8f9f-756707ba423c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(X_dummy_data,\n",
    "                                   vector_size=100, # size of the vector,\n",
    "                                                    # If feel you text messages have more tokens then increase the lenth\n",
    "                                   window=3, #`window` is the maximum distance between the current and predicted word \n",
    "                                             # within a sentence.\n",
    "                                   min_count=2 # ignore all words with total frequency lower than this\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2652c3fe-d3d5-4a99-8d77-14d67a7db56b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.28168786e-01,  2.94776231e-01,  5.42313941e-02,  3.35366116e-04,\n",
       "        1.37255648e-02, -6.58287704e-01,  1.82953089e-01,  8.47477376e-01,\n",
       "       -2.45199174e-01, -2.15501830e-01, -2.35695377e-01, -5.95315695e-01,\n",
       "       -1.27114728e-01,  2.03529745e-01,  1.32459402e-01, -3.54928523e-01,\n",
       "        5.92925549e-02, -5.11806726e-01, -1.64134875e-02, -7.97453165e-01,\n",
       "        2.51370192e-01,  2.02712864e-01, -6.05017506e-03, -1.01823099e-01,\n",
       "       -5.40905446e-02,  1.32755460e-02, -3.62832636e-01, -3.20576549e-01,\n",
       "       -3.55743676e-01,  4.59524542e-02,  4.24092948e-01,  7.39516020e-02,\n",
       "        5.42852655e-02, -1.32181421e-01, -9.67487916e-02,  4.10268098e-01,\n",
       "        3.39242704e-02, -3.58483493e-01, -2.31161505e-01, -7.17128217e-01,\n",
       "       -3.46506536e-02, -2.92194545e-01, -1.77535862e-01,  1.11085422e-01,\n",
       "        3.74869108e-01, -1.08029746e-01, -2.63971657e-01,  1.52991165e-03,\n",
       "        1.60804272e-01,  1.98200151e-01,  1.29610568e-01, -2.82718152e-01,\n",
       "       -1.12879865e-01, -1.43923955e-02, -2.00589657e-01,  2.35657483e-01,\n",
       "        2.34081119e-01, -1.90948561e-01, -3.75160873e-01,  8.86719227e-02,\n",
       "        1.59026265e-01,  1.84871331e-01, -1.28705084e-01, -1.06382288e-01,\n",
       "       -5.21857977e-01,  3.05691779e-01,  7.57297948e-02,  2.66881943e-01,\n",
       "       -4.54016536e-01,  4.78651106e-01, -2.07903519e-01,  2.06085399e-01,\n",
       "        4.23646271e-01, -1.70865431e-01,  4.08152193e-01,  2.88400143e-01,\n",
       "       -8.88208486e-03, -1.53847754e-01, -3.18799615e-01,  2.16293573e-01,\n",
       "       -1.46074638e-01, -3.67111377e-02, -3.33153546e-01,  6.43454015e-01,\n",
       "       -9.14677382e-02,  5.96414357e-02, -7.97934756e-02,  4.26786810e-01,\n",
       "        5.40748239e-01,  1.01394847e-01,  4.02704835e-01,  1.10056475e-01,\n",
       "        7.80150443e-02,  1.62502408e-01,  5.33864141e-01,  3.82748395e-01,\n",
       "        1.32935926e-01, -4.34082508e-01,  1.06610745e-01, -5.18772416e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed935812-7b3f-4b73-9d91-3fbdec35170b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0, 'better': 1, 'good': 2, 'nothing': 3, 'than': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efe39a39-3ba3-45f2-8566-fec8599848b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c866a8-137e-4cdb-b054-831e07d6e3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'better', 'good', 'nothing', 'than']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e235a7-67cd-482e-9955-07d06168cf69",
   "metadata": {},
   "source": [
    "#### Applying vectoring model on our spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75916cd1-beab-4757-b064-cf54a5a40822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "messages_df = messages_df.drop(labels=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "messages_df.columns = [\"label\", \"text\"]\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17417644-182e-4bdc-b28f-4bfb4b44ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point, crazy.. available bugis n great ...</td>\n",
       "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun early hor... u c say...</td>\n",
       "      <td>[dun, early, hor, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah don't think goes usf, lives</td>\n",
       "      <td>[nah, don, think, goes, usf, lives]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text   \n",
       "0   ham  jurong point, crazy.. available bugis n great ...  \\\n",
       "1   ham                      ok lar... joking wif u oni...   \n",
       "2  spam  free entry 2 wkly comp win fa cup final tkts 2...   \n",
       "3   ham                      u dun early hor... u c say...   \n",
       "4   ham                    nah don't think goes usf, lives   \n",
       "\n",
       "                                          text_clean  \n",
       "0  [jurong, point, crazy, available, bugis, great...  \n",
       "1                        [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
       "3                             [dun, early, hor, say]  \n",
       "4                [nah, don, think, goes, usf, lives]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a document into a list of tokens.\n",
    "# This lowercases, tokenizes, de-accents (optional). – the output are final\n",
    "# tokens = unicode strings, that won’t be processed any further.\n",
    "messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x.lower()))\n",
    "messages_df[\"text_clean\"] = messages_df[\"text\"].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ea61ca0-c4da-483a-b601-4ae44714f723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages_df[\"text_clean\"], \n",
    "                                                    messages_df[\"label\"],\n",
    "                                                    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c0d0b07-b7af-4edf-b252-95d38062eb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Documenation: https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100, # 'size' of the vector,\n",
    "                                   window=3,        #'window' is the maximum distance between the current and predicted word \n",
    "                                                    # within a sentence.\n",
    "                                   min_count=2      #'min_count' ignore all words with total frequency lower than this\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "721db16a-019c-44fa-a87f-d8ae6436c39d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3327, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it has found around 2236 unique words from all sentence \n",
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ceef6201-b94c-4d74-8098-47b8fba126fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01540353,  0.02704115,  0.00047133,  0.00184532,  0.00344139,\n",
       "       -0.04279326,  0.00590101,  0.05406338, -0.0152276 , -0.01037676,\n",
       "       -0.00672873, -0.03237762, -0.00977162,  0.01117991,  0.00303108,\n",
       "       -0.01064968,  0.00362202, -0.03314536, -0.00059913, -0.05471944,\n",
       "        0.0086899 ,  0.02047253, -0.00240871, -0.00359745, -0.01171398,\n",
       "        0.01361787, -0.03178739, -0.02237112, -0.02020619,  0.00915507,\n",
       "        0.01671531,  0.00733537,  0.00442236, -0.00340522,  0.00493995,\n",
       "        0.0342346 ,  0.00730813, -0.02075255, -0.00889054, -0.05365228,\n",
       "        0.00112327, -0.02593256, -0.00352555,  0.00921736,  0.01512694,\n",
       "       -0.02015343, -0.01613283,  0.00396995,  0.01825234,  0.01231018,\n",
       "        0.01598307, -0.02147548, -0.00420659, -0.01027708, -0.01967317,\n",
       "        0.02323181,  0.00934186, -0.0069949 , -0.02478185, -0.00246385,\n",
       "       -0.00148238,  0.01543016,  0.00652061,  0.01073922, -0.02576259,\n",
       "        0.02429629,  0.01407668,  0.02275086, -0.03823524,  0.03307063,\n",
       "       -0.01286115,  0.00261444,  0.02367801, -0.0143819 ,  0.03172622,\n",
       "        0.02714591,  0.00589062, -0.01077801, -0.02108987,  0.0038622 ,\n",
       "       -0.0172509 , -0.01223044, -0.02890011,  0.03618692, -0.00665824,\n",
       "        0.00502551, -0.00388996,  0.02365565,  0.01741809,  0.01323291,\n",
       "        0.03135076,  0.02309639,  0.01705901,  0.02107752,  0.045151  ,\n",
       "        0.02230828,  0.0174636 , -0.03237936,  0.00836803, -0.00132267],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"king\"]shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1620cf55-8518-4e14-bce0-d854eb2ecad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lol', 0.9684240818023682),\n",
       " ('dis', 0.9677614569664001),\n",
       " ('maybe', 0.9675440788269043),\n",
       " ('told', 0.9673854112625122),\n",
       " ('minutes', 0.9673014283180237)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ofcourse it learned very badly as compared with pretrained glove-wiki-gigaword-100 model\n",
    "w2v_model.wv.most_similar(\"king\", \n",
    "                          topn=5, # Number of example\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "354b69ea-8e46-4a1e-87a9-27da1cafec27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 100)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which it it able to find 2232 which are repreated 3 times and created 100 elements vector for each word.\n",
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7ab325c-85f4-405a-b2ed-0696c79499a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate aggregated sentence vectors based on the word vectors for each word in the sentence\n",
    "word_vec = [np.array([w2v_model.wv[i] for i in ls if i in w2v_model.wv.index_to_key])\n",
    "          for ls in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9bd26ac0-8bd2-4245-8273-e7366749ae89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 6\n",
      "12 3\n",
      "34 11\n",
      "4 18\n",
      "5 3\n",
      "15 7\n",
      "4 1\n",
      "6 2\n",
      "6 3\n",
      "6 14\n"
     ]
    }
   ],
   "source": [
    "# Checking words in the sentence and number of vectors created for it. Both has to match. if doesn't\n",
    "# Then we have to fix it.\n",
    "count = 0\n",
    "for i, v in enumerate(word_vec):\n",
    "    if count == 10:\n",
    "        break\n",
    "    print(len(X_test.iloc[i]), len(v))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7023f4d2-1229-465a-b4c1-c419fd7b9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect_avg = []\n",
    "for vect in word_vec:\n",
    "    if len(vect) != 0:\n",
    "        w2v_vect_avg.append(vect.mean(axis=0))\n",
    "    else:\n",
    "        w2v_vect_avg.append(np.zeros(100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "80aeda5d-fdec-4cd3-bbfc-14d2d030e55e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 100\n",
      "12 100\n",
      "34 100\n",
      "4 100\n",
      "5 100\n",
      "15 100\n",
      "4 100\n",
      "6 100\n",
      "6 100\n",
      "6 100\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, v in enumerate(w2v_vect_avg):\n",
    "    if count == 10:\n",
    "        break\n",
    "    print(len(X_test.iloc[i]), len(v))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aaf0e5-71df-47e5-94f4-91055d83ab71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "770031eb-2207-4103-825b-c04c6c9cd343",
   "metadata": {},
   "source": [
    "### Resources\n",
    "1. https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
