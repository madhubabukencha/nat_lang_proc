{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47852849-02ca-4da8-89e6-f2dfcfeebf99",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Word2Vect</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923f29c-48ed-4a76-8881-22c171e2dfcc",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Word2Vec is a shallow,two-layer neural network that accepts a text corpus as an input and returns a set\n",
    "       of vectors(also know as embedding). Each vector is a neumeric representation of a given word. It is capable\n",
    "       of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "       Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving \n",
    "       Neural Networks): Skip Gram and Common Bag Of Words (CBOW).\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c18b9-04dd-476b-bbce-bb18c81c1b5d",
   "metadata": {},
   "source": [
    "### Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a3c389-12ed-4b6e-a506-da0c22a4e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87976f0-fdc9-45ef-8422-9785ce5e7664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained word vectors using gensim\n",
    "# Other pretrained model can found at below link\n",
    "# https://github.com/RaRe-Technologies/gensim-data\n",
    "wiki_embeddings = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1309591f-628f-4b4e-998e-b6b36d9516de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the word vector king\n",
    "wiki_embeddings[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49330c3b-14e8-4404-be8d-2d764378c80b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7682328820228577),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919989585876465),\n",
       " ('kingdom', 0.6811409592628479),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712858080863953),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the words most similar to king based on trained word vector\n",
    "wiki_embeddings.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91031aea-ae5c-48e8-8ae6-451358fe987c",
   "metadata": {},
   "source": [
    "### Training our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75916cd1-beab-4757-b064-cf54a5a40822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "messages_df = messages_df.drop(labels=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "messages_df.columns = [\"label\", \"text\"]\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17417644-182e-4bdc-b28f-4bfb4b44ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point, crazy.. available bugis n great ...</td>\n",
       "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun early hor... u c say...</td>\n",
       "      <td>[dun, early, hor, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah don't think goes usf, lives</td>\n",
       "      <td>[nah, don, think, goes, usf, lives]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text   \n",
       "0   ham  jurong point, crazy.. available bugis n great ...  \\\n",
       "1   ham                      ok lar... joking wif u oni...   \n",
       "2  spam  free entry 2 wkly comp win fa cup final tkts 2...   \n",
       "3   ham                      u dun early hor... u c say...   \n",
       "4   ham                    nah don't think goes usf, lives   \n",
       "\n",
       "                                          text_clean  \n",
       "0  [jurong, point, crazy, available, bugis, great...  \n",
       "1                        [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
       "3                             [dun, early, hor, say]  \n",
       "4                [nah, don, think, goes, usf, lives]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a document into a list of tokens.\n",
    "# This lowercases, tokenizes, de-accents (optional). – the output are final\n",
    "# tokens = unicode strings, that won’t be processed any further.\n",
    "messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x.lower()))\n",
    "messages_df[\"text_clean\"] = messages_df[\"text\"].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c0d0b07-b7af-4edf-b252-95d38062eb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages_df[\"text_clean\"], \n",
    "                                                    messages_df[\"label\"],\n",
    "                                                    test_size=0.20)\n",
    "# Documenation: https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100, # size of the vector\n",
    "                                   window=3, #`window` is the maximum distance between the current and predicted word within a sentence.\n",
    "                                   min_count=3 # ignore all words with total frequency lower than this\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ceef6201-b94c-4d74-8098-47b8fba126fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01038788,  0.01430211, -0.00174529,  0.00702417,  0.01507522,\n",
       "       -0.04153308,  0.01270943,  0.0454486 , -0.01883297, -0.0072866 ,\n",
       "        0.00134877, -0.03388841, -0.01434648,  0.00882359,  0.00890319,\n",
       "       -0.02159827,  0.00762409, -0.01556034, -0.00138938, -0.03971127,\n",
       "        0.01525787,  0.00120921,  0.01363898, -0.01067892,  0.00040899,\n",
       "        0.00362042, -0.01863683, -0.01215883, -0.02106537, -0.00254103,\n",
       "        0.02084519,  0.00406907,  0.0015666 , -0.00930545,  0.00360219,\n",
       "        0.02288773,  0.00624533, -0.01466221, -0.00341158, -0.02754484,\n",
       "       -0.00296511, -0.02049479, -0.00642643,  0.00969344,  0.01968034,\n",
       "       -0.01694279, -0.01895383, -0.01204172,  0.00993854,  0.02615556,\n",
       "       -0.00333178, -0.00763972, -0.01172165,  0.00725493, -0.00563524,\n",
       "        0.01652205,  0.00522912, -0.00728336, -0.01672616,  0.00798486,\n",
       "        0.01129118,  0.00052489, -0.00071299, -0.00167633, -0.01569651,\n",
       "        0.01471573,  0.00684515,  0.00674684, -0.02761444,  0.01070102,\n",
       "       -0.0077855 ,  0.00905564,  0.02263494, -0.01482389,  0.01973699,\n",
       "       -0.00159497, -0.002917  ,  0.00583284, -0.01701256,  0.0074369 ,\n",
       "       -0.00212547,  0.00967502, -0.01778173,  0.02823818, -0.00785888,\n",
       "       -0.0055062 ,  0.01171869,  0.02077402,  0.02217222,  0.00952518,\n",
       "        0.02736446,  0.01374192, -0.00020533, -0.00242859,  0.0247857 ,\n",
       "        0.01824788,  0.01743811, -0.01820435,  0.01804132,  0.00321944],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1620cf55-8518-4e14-bce0-d854eb2ecad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ur', 0.9992110729217529),\n",
       " ('txt', 0.9991639852523804),\n",
       " ('like', 0.9991474747657776),\n",
       " ('me', 0.99908846616745),\n",
       " ('now', 0.9990391135215759),\n",
       " ('said', 0.9990137219429016),\n",
       " ('reply', 0.998992383480072),\n",
       " ('new', 0.9989880323410034),\n",
       " ('want', 0.998987078666687),\n",
       " ('know', 0.9989732503890991)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ofcourse it learned very badly\n",
    "w2v_model.wv.most_similar(\"come\", \n",
    "                          topn=10, # Number of example\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770031eb-2207-4103-825b-c04c6c9cd343",
   "metadata": {},
   "source": [
    "### Resources\n",
    "1. https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330aa48-91e5-463c-ae1e-5d1ac5776830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
