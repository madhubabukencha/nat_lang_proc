{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47852849-02ca-4da8-89e6-f2dfcfeebf99",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Word2Vect</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923f29c-48ed-4a76-8881-22c171e2dfcc",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Word2Vec is a shallow,two-layer neural network that accepts a text corpus as an input and returns a set\n",
    "       of vectors(also know as embedding). Each vector is a neumeric representation of a given word. It is capable\n",
    "       of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "       Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving \n",
    "       Neural Networks): Skip Gram and Common Bag Of Words (CBOW).\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c18b9-04dd-476b-bbce-bb18c81c1b5d",
   "metadata": {},
   "source": [
    "### Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a3c389-12ed-4b6e-a506-da0c22a4e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87976f0-fdc9-45ef-8422-9785ce5e7664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading pretrained word vectors using gensim\n",
    "# Other pretrained model can found at below link\n",
    "# https://github.com/RaRe-Technologies/gensim-data\n",
    "wiki_embeddings = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1309591f-628f-4b4e-998e-b6b36d9516de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the word vector king\n",
    "wiki_embeddings[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49330c3b-14e8-4404-be8d-2d764378c80b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7682328820228577),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919989585876465),\n",
       " ('kingdom', 0.6811409592628479),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712858080863953),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the words most similar to king based on trained word vector\n",
    "wiki_embeddings.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91031aea-ae5c-48e8-8ae6-451358fe987c",
   "metadata": {},
   "source": [
    "### Training our own vectorizing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75ba46-fd1a-4089-8858-75cdda850942",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Understanding with simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "647459b6-2213-4e52-937b-550e7113e679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_dummy_data = pd.Series([[\"something\", \"is\", \"better\", \"than\", \"nothing\"], \n",
    "                          [\"good\", \"is\", \"better\", \"than\", \"nothing\"],\n",
    "                          [\"Love\", \"is\", \"good\", \"but\", \"break\",\"up\", \"is\",\"better\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d68becac-0087-4103-994a-178501c05cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [something, is, better, than, nothing]\n",
       "1               [good, is, better, than, nothing]\n",
       "2    [Love, is, good, but, break, up, is, better]\n",
       "dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "100f2571-d6e1-4cf2-8f9f-756707ba423c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(X_dummy_data,\n",
    "                                   vector_size=100, # size of the vector,\n",
    "                                                    # If feel you text messages have more tokens then increase the lenth\n",
    "                                   window=3, #`window` is the maximum distance between the current and predicted word \n",
    "                                             # within a sentence.\n",
    "                                   min_count=2 # ignore all words with total frequency lower than this\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed935812-7b3f-4b73-9d91-3fbdec35170b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0, 'better': 1, 'good': 2, 'nothing': 3, 'than': 4}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "efe39a39-3ba3-45f2-8566-fec8599848b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e235a7-67cd-482e-9955-07d06168cf69",
   "metadata": {},
   "source": [
    "#### Applying vectoring model on our spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75916cd1-beab-4757-b064-cf54a5a40822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "messages_df = messages_df.drop(labels=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "messages_df.columns = [\"label\", \"text\"]\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17417644-182e-4bdc-b28f-4bfb4b44ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point, crazy.. available bugis n great ...</td>\n",
       "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun early hor... u c say...</td>\n",
       "      <td>[dun, early, hor, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah don't think goes usf, lives</td>\n",
       "      <td>[nah, don, think, goes, usf, lives]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text   \n",
       "0   ham  jurong point, crazy.. available bugis n great ...  \\\n",
       "1   ham                      ok lar... joking wif u oni...   \n",
       "2  spam  free entry 2 wkly comp win fa cup final tkts 2...   \n",
       "3   ham                      u dun early hor... u c say...   \n",
       "4   ham                    nah don't think goes usf, lives   \n",
       "\n",
       "                                          text_clean  \n",
       "0  [jurong, point, crazy, available, bugis, great...  \n",
       "1                        [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
       "3                             [dun, early, hor, say]  \n",
       "4                [nah, don, think, goes, usf, lives]  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a document into a list of tokens.\n",
    "# This lowercases, tokenizes, de-accents (optional). – the output are final\n",
    "# tokens = unicode strings, that won’t be processed any further.\n",
    "messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x.lower()))\n",
    "messages_df[\"text_clean\"] = messages_df[\"text\"].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9ea61ca0-c4da-483a-b601-4ae44714f723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages_df[\"text_clean\"], \n",
    "                                                    messages_df[\"label\"],\n",
    "                                                    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8c0d0b07-b7af-4edf-b252-95d38062eb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Documenation: https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100, # 'size' of the vector,\n",
    "                                                    # If feel you text messages have more tokens then increase the lenth\n",
    "                                   window=3,        #'window' is the maximum distance between the current and predicted word \n",
    "                                                    # within a sentence.\n",
    "                                   min_count=3      #'min_count' ignore all words with total frequency lower than this\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ceef6201-b94c-4d74-8098-47b8fba126fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.36129437e-02,  1.76094379e-02,  1.56155077e-03, -3.53098987e-03,\n",
       "        1.32325087e-02, -2.83463877e-02,  1.37087665e-02,  3.64526212e-02,\n",
       "       -1.69212036e-02, -1.99795049e-02, -2.38756370e-03, -3.16845737e-02,\n",
       "        1.64627004e-03,  9.85298213e-03,  4.04103484e-05, -1.41845420e-02,\n",
       "        3.15313693e-03, -2.30012406e-02, -8.58663209e-03, -5.16244248e-02,\n",
       "        1.34838400e-02, -1.09210867e-03, -3.39793344e-03, -3.59541550e-03,\n",
       "       -9.28980298e-03, -6.90292520e-03, -1.27994083e-02, -2.62981653e-02,\n",
       "       -2.69380827e-02,  8.98756739e-03,  3.11183818e-02, -1.76429551e-03,\n",
       "        3.53080616e-03, -1.62198786e-02, -6.05623564e-03,  2.85396427e-02,\n",
       "        5.89518622e-03, -1.42099345e-02, -1.56633798e-02, -4.96917777e-02,\n",
       "        1.86526054e-03, -2.25843303e-02,  4.43299767e-03,  6.35256385e-03,\n",
       "        1.95551198e-02, -9.88571439e-03, -1.19930981e-02, -4.99160355e-03,\n",
       "        5.92284417e-03,  1.60462763e-02, -5.70295798e-03, -1.29251266e-02,\n",
       "       -1.97508442e-03, -1.01048080e-02, -1.99439060e-02,  2.74604070e-03,\n",
       "        2.19971649e-02, -7.74649496e-04, -2.58290526e-02,  2.79602362e-04,\n",
       "        6.26793085e-03,  2.79046455e-03, -7.73604587e-03,  9.78165120e-03,\n",
       "       -1.12779438e-02,  1.40068205e-02, -1.40020985e-03,  1.50595354e-02,\n",
       "       -1.75186228e-02,  2.91883256e-02, -1.34429364e-02,  2.19612718e-02,\n",
       "        1.96167398e-02, -1.87856387e-02,  1.73276737e-02,  1.27242990e-02,\n",
       "        9.56758112e-03,  3.94069450e-03, -1.82401612e-02,  1.23651847e-02,\n",
       "       -1.83578134e-02,  9.05311853e-03, -1.77647006e-02,  2.04024203e-02,\n",
       "       -4.50028060e-03, -1.62378245e-03, -6.45468570e-03,  2.50309333e-02,\n",
       "        2.26678457e-02,  5.63233905e-03,  3.11551131e-02,  1.37203224e-02,\n",
       "       -1.77167519e-03,  1.20043084e-02,  3.24074216e-02,  1.37281287e-02,\n",
       "        1.85013227e-02, -2.95505524e-02,  1.29401535e-02, -1.39593035e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1620cf55-8518-4e14-bce0-d854eb2ecad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goin', 0.9480611681938171),\n",
       " ('am', 0.9469554424285889),\n",
       " ('call', 0.9462488293647766),\n",
       " ('takes', 0.9462072849273682),\n",
       " ('muz', 0.9460297226905823)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ofcourse it learned very badly as compared with pretrained glove-wiki-gigaword-100 model\n",
    "w2v_model.wv.most_similar(\"king\", \n",
    "                          topn=5, # Number of example\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "354b69ea-8e46-4a1e-87a9-27da1cafec27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2232, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which it it able to find 2232 which are repreated 3 times and created 100 elements vector for each word.\n",
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e14c89-caa1-4540-82cc-ad85133d332a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3063b2-3250-437f-80b7-88581af2a220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023f4d2-1229-465a-b4c1-c419fd7b9b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f503192-1a25-4625-98d9-8ee1a3f33561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fd92a-f377-410a-b1ea-7a66843f61a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "770031eb-2207-4103-825b-c04c6c9cd343",
   "metadata": {},
   "source": [
    "### Resources\n",
    "1. https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330aa48-91e5-463c-ae1e-5d1ac5776830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36459a1-d70c-4d4e-af3b-77734d15b26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
